{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "KAL9Mz6sHYo7",
      "metadata": {
        "id": "KAL9Mz6sHYo7"
      },
      "outputs": [],
      "source": [
        "# batch size\n",
        "# top k\n",
        "# temperature\n",
        "# early stop\n",
        "# regularization\n",
        "\n",
        "WEIGHT_DECAY = 1e-4\n",
        "PATIENCE = 3\n",
        "TOP_K = 50\n",
        "TEMPERATURE = 1\n",
        "\n",
        "# PATH_RES = '/content/drive/MyDrive/rnn_results_1'\n",
        "# PATH_RES = '/content/drive/MyDrive/test_demo_detokenize_earlystop'\n",
        "# PATH_RES = 'test_demo_temp1_nodecay'\n",
        "# PATH_RES = 'test_demo_weight_decay1e-4_temp1'\n",
        "PATH_RES = 'layer=3/test_demo_weight_decay1e-4_temp1'\n",
        "# PATH_RES = 'layer=2/test_demo_weight_decay1e-4_temp0.7'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ZaOEMi4FHZgG",
      "metadata": {
        "id": "ZaOEMi4FHZgG"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117f13c2",
      "metadata": {
        "id": "117f13c2"
      },
      "source": [
        "# RNN (LSTM) Next-Word Prediction on tiny_shakespeare\n",
        "### Features implemented:\n",
        "- Load dataset from Hugging Face: karpathy/tiny_shakespeare\n",
        "- Word-level tokenization (simple regex-based)\n",
        "- Build vocabulary, dataset of (context -> next_word) using sliding window\n",
        "- PyTorch LSTM model with nn.Embedding\n",
        "- Training loop with validation split, logging and plotting (matplotlib)\n",
        "- Perplexity and token-level accuracy calculation\n",
        "- Text generation given a seed phrase (generates N words)\n",
        "- Ablation study: run two experiments with different hidden sizes and compare curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d58599b",
      "metadata": {
        "id": "9d58599b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\semester 7\\gen ai\\ass1\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5RPkSAPe1Lrn",
      "metadata": {
        "id": "5RPkSAPe1Lrn"
      },
      "source": [
        "### Tokenize text\n",
        "Tokenizes text into words while keeping punctuation as separate tokens.Uses a simple regex-based approach for educational purposes.\n",
        "\n",
        "**Parameters**:\n",
        "*   text (str): Input text to tokenize\n",
        "\n",
        "**Returns**:\n",
        "*   list: List of tokens (words and punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fd109396",
      "metadata": {
        "id": "fd109396"
      },
      "outputs": [],
      "source": [
        "\n",
        "def simple_word_tokenize(text):\n",
        "    # split by whitespace, but separate punctuation\n",
        "    tokens = re.findall(r\"\\w+|[^\\\\s\\w]\", text)\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7ec00d",
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "STOP_WORDS = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "vSByv5Xh5oTi",
      "metadata": {
        "id": "vSByv5Xh5oTi"
      },
      "outputs": [],
      "source": [
        "def detokenize(tokens):\n",
        "    text = \"\"\n",
        "    for i, tok in enumerate(tokens):\n",
        "        if i > 0 and tok not in [\".\", \",\", \"?\", \"!\", \";\", \":\", \"'\", '\"']:\n",
        "            text += \" \"\n",
        "        text += tok\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ergWx9HP7mK-",
      "metadata": {
        "id": "ergWx9HP7mK-"
      },
      "outputs": [],
      "source": [
        "def compute_perplexity(loss):\n",
        "    # loss is average negative log-likelihood\n",
        "    return math.exp(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02hDg7av6H7P",
      "metadata": {
        "id": "02hDg7av6H7P"
      },
      "source": [
        "### Build Vocab\n",
        "Builds vocabulary from tokens with frequency filtering and optional size limiting.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "*   tokens (list): List of tokens to build vocabulary from\n",
        "    \n",
        "*   min\\_freq (int, optional): Minimum frequency for tokens to be included. Defaults to 1\n",
        "    \n",
        "*   max\\_size (int, optional): Maximum vocabulary size. Defaults to None (no limit)\n",
        "    \n",
        "\n",
        "**Returns**:\n",
        "\n",
        "*   tuple: (token\\_to\\_idx, idx\\_to\\_token) - Dictionary mapping tokens to indices and list mapping indices to tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8a6f3911",
      "metadata": {
        "id": "8a6f3911"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_vocab(tokens, min_freq=1, max_size=None):\n",
        "    #Counts frequency of all tokens in the dataset.\n",
        "    counter = Counter(tokens)\n",
        "    # keep tokens with freq >= min_freq\n",
        "    items = [(tok, cnt) for tok, cnt in counter.items() if cnt >= min_freq]\n",
        "    items.sort(key=lambda x: (-x[1], x[0]))\n",
        "    # Optionally limits vocab size (max_size)\n",
        "    if max_size:\n",
        "        items = items[:max_size]\n",
        "    idx_to_token = [\"<pad>\", \"<unk>\"] + [tok for tok, _ in items]\n",
        "    token_to_idx = {tok: i for i, tok in enumerate(idx_to_token)}\n",
        "    return token_to_idx, idx_to_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GkJVyxLy6WR8",
      "metadata": {
        "id": "GkJVyxLy6WR8"
      },
      "source": [
        "### Tokens to training samples\n",
        "PyTorch Dataset for next word prediction tasks. Creates input-target pairs from token sequences.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "*   token\\_idxs (list): List of integer token IDs\n",
        "    \n",
        "*   context\\_size (int): Size of the context window for prediction\n",
        "    \n",
        "\n",
        "**Methods**:\n",
        "\n",
        "*   \\_\\_len\\_\\_(): Returns the number of samples in the dataset\n",
        "    \n",
        "*   \\_\\_getitem\\_\\_(idx): Returns (input\\_context, target\\_word) tuple for the given index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7d2978cf",
      "metadata": {
        "id": "7d2978cf"
      },
      "outputs": [],
      "source": [
        "class NextWordDataset(Dataset):\n",
        "    def __init__(self, token_idxs, context_size):\n",
        "        # token_idxs: list of integer token ids\n",
        "        self.context_size = context_size\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        # Turns the sequence of token indices into training examples.\n",
        "        for i in range(len(token_idxs) - context_size):\n",
        "            ctx = token_idxs[i : i + context_size]\n",
        "            targ = token_idxs[i + context_size]\n",
        "            self.inputs.append(torch.tensor(ctx, dtype=torch.long))\n",
        "            self.targets.append(torch.tensor(targ, dtype=torch.long))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1-68ttX36lXo",
      "metadata": {
        "id": "1-68ttX36lXo"
      },
      "source": [
        "### Prepare Dataset\n",
        "Prepares the Tiny Shakespeare dataset for next word prediction training.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "*   context\\_size (int, optional): Size of context window. Defaults to 5\n",
        "    \n",
        "*   val\\_split (float, optional): Fraction of data to use for validation. Defaults to 0.1\n",
        "    \n",
        "\n",
        "**Returns**:\n",
        "\n",
        "*   tuple: (train\\_ds, val\\_ds, token\\_to\\_idx, idx\\_to\\_token) - Training dataset, validation dataset, and vocabulary mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4845e08f",
      "metadata": {
        "id": "4845e08f"
      },
      "outputs": [],
      "source": [
        "def download_tiny_shakespeare():\n",
        "\turl = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\ttext = requests.get(url).text\n",
        "\treturn text\n",
        "\n",
        "\n",
        "def prepare_dataset(context_size=5, val_split=0.1):\n",
        "\ttext = download_tiny_shakespeare()\n",
        "\n",
        "  \t# Tokenizes text → splits into words.\n",
        "\ttokens = simple_word_tokenize(text)\n",
        " \t# Lowercase tokens\n",
        "\ttokens = [t.lower() for t in tokens]\n",
        "\t# Remove stop words\n",
        "\ttokens = [t for t in tokens if t not in STOP_WORDS]\n",
        "\t\n",
        "\t# mappings of token to id and back\n",
        "\ttoken_to_idx, idx_to_token = build_vocab(tokens, min_freq=1)\n",
        "\tprint(f\"Vocab size: {len(idx_to_token)}\")\n",
        "\ttoken_idxs = [token_to_idx.get(t, token_to_idx['<unk>']) for t in tokens]\n",
        "\t# windows of context for dataset usage\n",
        "\tdataset = NextWordDataset(token_idxs, context_size=context_size)\n",
        "\n",
        "\t# split train & test\n",
        "\tn_val = int(val_split * len(dataset)) # 10%\n",
        "\tn_train = len(dataset) - n_val # 90%\n",
        "\ttrain_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "\n",
        "\treturn train_ds, val_ds, token_to_idx, idx_to_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "STl8Cp0Q76qe",
      "metadata": {
        "id": "STl8Cp0Q76qe"
      },
      "source": [
        "### Model Construction\n",
        "LSTM-based neural network model for next word prediction tasks.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "*   vocab\\_size (int): Size of the vocabulary\n",
        "    \n",
        "*   emb\\_size (int): Dimensionality of word embeddings\n",
        "    \n",
        "*   hidden\\_size (int): Number of features in the hidden state of LSTM\n",
        "    \n",
        "*   num\\_layers (int, optional): Number of recurrent layers. Defaults to 1\n",
        "    \n",
        "*   dropout (float, optional): Dropout probability. Defaults to 0.0\n",
        "    \n",
        "\n",
        "**Forward Pass**:\n",
        "\n",
        "*   Input: x (Tensor) - Batch of token sequences with shape (batch\\_size, context\\_size)\n",
        "    \n",
        "*   Output: logits (Tensor) - Unnormalized scores for each vocabulary word with shape (batch\\_size, vocab\\_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b05f0cc9",
      "metadata": {
        "id": "b05f0cc9"
      },
      "outputs": [],
      "source": [
        "class LSTMNextWordModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        # create vector embedding for each word; word -> vector space\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        # hidden layer size -> logits for each work in vocab (scores)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, context_size) -> batch\n",
        "        emb = self.embedding(x)  # (batch, context, emb)\n",
        "        out, _ = self.lstm(emb)  # out: (batch, context, hidden)\n",
        "        last = out[:, -1, :]     # take last timestep atfer all context window finished\n",
        "        logits = self.fc(last)   # (batch, vocab) not normalized\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vsi8myUv8LYm",
      "metadata": {
        "id": "vsi8myUv8LYm"
      },
      "source": [
        "### Train 1 epoch (batch)\n",
        "\n",
        "Trains the model for one epoch on the provided dataloader.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "*   model (nn.Module): The neural network model to train\n",
        "    \n",
        "*   dataloader (DataLoader): DataLoader providing training batches\n",
        "    \n",
        "*   criterion (nn.Module): Loss function (e.g., CrossEntropyLoss)\n",
        "    \n",
        "*   optimizer (torch.optim.Optimizer): Optimization algorithm\n",
        "    \n",
        "*   device (str): Device to run training on ('cpu' or 'cuda')\n",
        "    \n",
        "\n",
        "**Returns**:\n",
        "\n",
        "*   tuple: (avg\\_loss, accuracy) - Average loss and accuracy for the epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "566fb689",
      "metadata": {
        "id": "566fb689"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "    for x, y in dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # clears grad for each batch\n",
        "        optimizer.zero_grad()\n",
        "        # logits = vector of scores for each word in vocab\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # total tokens in batch * loss per item\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        # pick highest val in vector\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        total_correct += (preds == y).sum().item()\n",
        "        total_tokens += x.size(0)\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    acc = total_correct / total_tokens\n",
        "    return avg_loss, acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GP86CB0r8pt_",
      "metadata": {
        "id": "GP86CB0r8pt_"
      },
      "source": [
        "### Evaluate\n",
        " Evaluates the model on the provided dataloader without training.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "*   model (nn.Module): The neural network model to evaluate\n",
        "    \n",
        "*   dataloader (DataLoader): DataLoader providing evaluation batches\n",
        "    \n",
        "*   criterion (nn.Module): Loss function\n",
        "    \n",
        "*   device (str): Device to run evaluation on ('cpu' or 'cuda')\n",
        "    \n",
        "\n",
        "**Returns**:\n",
        "\n",
        "*   tuple: (avg\\_loss, accuracy) - Average loss and accuracy for the evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8d7262fb",
      "metadata": {
        "id": "8d7262fb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # logits = vector of scores for each word in vocab\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            # pick highest val in vector\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            total_correct += (preds == y).sum().item()\n",
        "            total_tokens += x.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    acc = total_correct / total_tokens\n",
        "    return avg_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZuQS6fhd8wyj",
      "metadata": {
        "id": "ZuQS6fhd8wyj"
      },
      "source": [
        "### Generate Text - use model\n",
        "\n",
        " Generates text using the trained model with various sampling strategies.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "*   model (nn.Module): Trained language model\n",
        "    \n",
        "*   token\\_to\\_idx (dict): Vocabulary mapping from tokens to indices\n",
        "    \n",
        "*   idx\\_to\\_token (list): Vocabulary mapping from indices to tokens\n",
        "    \n",
        "*   seed\\_text (str): Initial text to start generation from\n",
        "    \n",
        "*   gen\\_len (int, optional): Number of tokens to generate. Defaults to 10\n",
        "    \n",
        "*   context\\_size (int, optional): Context window size. Defaults to 5\n",
        "    \n",
        "*   device (str, optional): Device to run generation on. Defaults to 'cpu'\n",
        "    \n",
        "*   temperature (float, optional): Controls randomness (higher = more random). Defaults to 1.0\n",
        "    \n",
        "*   top\\_k (int, optional): Limits sampling to top-k most likely tokens. Defaults to None\n",
        "    \n",
        "\n",
        "**Returns**:\n",
        "\n",
        "*   str: Generated text sequence\n",
        "    \n",
        "\n",
        "**Sampling Strategies**:\n",
        "\n",
        "*   temperature: Adjusts probability distribution (higher values increase diversity)\n",
        "    \n",
        "*   top\\_k: Restricts sampling to the k most likely tokens for more coherent output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9388660c",
      "metadata": {
        "id": "9388660c"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, token_to_idx, idx_to_token, seed_text, gen_len=10, context_size=5, device='cpu', temperature=1.0, top_k=None):\n",
        "    model.eval()\n",
        "    # Convert seed phrase into token IDs\n",
        "    toks = simple_word_tokenize(seed_text)\n",
        "    # lowercase\n",
        "    toks = [t.lower() for t in toks]\n",
        "    # map to ids (unk for unknown)\n",
        "    ids = [token_to_idx.get(t, token_to_idx.get('<unk>')) for t in toks]\n",
        "\n",
        "    # Pad or truncate to fit context_size\n",
        "    while len(ids) < context_size:\n",
        "        ids = [token_to_idx.get('<pad>')] + ids\n",
        "    ids = ids[-context_size:]\n",
        "\n",
        "    generated = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(gen_len):\n",
        "            x = torch.tensor([ids], dtype=torch.long, device=device)\n",
        "            logits = model(x)  # (1, vocab) x->token\n",
        "            logits = logits.squeeze(0) / max(1e-8, temperature)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            if top_k is not None:\n",
        "                # removes unlikely words\n",
        "                topk_vals, topk_idx = torch.topk(probs, k=top_k)\n",
        "                topk_probs = topk_vals / torch.sum(topk_vals)\n",
        "                # chosen 50 top words most likely\n",
        "                chosen = np.random.choice(topk_idx.cpu().numpy(), p=topk_probs.cpu().numpy())\n",
        "            else:\n",
        "                chosen = np.random.choice(len(probs.cpu().numpy()), p=probs.cpu().numpy())\n",
        "\n",
        "            generated.append(idx_to_token[chosen])\n",
        "            # drop oldest token in context + add new\n",
        "            ids = ids[1:] + [chosen]\n",
        "    return detokenize(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wxFkuPbyGxIK",
      "metadata": {
        "id": "wxFkuPbyGxIK"
      },
      "source": [
        "### Run Single Experiment\n",
        "Comprehensive training and evaluation pipeline for the LSTM next-word prediction model. This function orchestrates the entire machine learning workflow from data preparation to model training, evaluation, and result analysis.**Workflow**:\n",
        "\n",
        "1.  **Data Preparation**: Loads and preprocesses the Tiny Shakespeare dataset, builds vocabulary, and creates DataLoader objects for training and validation\n",
        "    \n",
        "2.  **Model Setup**: Initializes the LSTM model with specified architecture parameters and sets up loss function and optimizer\n",
        "    \n",
        "3.  **Training Loop**: Executes training for multiple epochs with batch processing, gradient updates, and performance tracking\n",
        "    \n",
        "4.  **Early Stopping**: Monitors validation loss and stops training if no improvement is detected for several consecutive epochs\n",
        "    \n",
        "5.  **Evaluation**: Computes final validation perplexity as a language modeling quality metric\n",
        "    \n",
        "6.  **Visualization**: Generates and saves loss and accuracy curves to monitor training progress\n",
        "    \n",
        "7.  **Text Generation**: Demonstrates model capabilities by generating text from a seed phrase\n",
        "    \n",
        "8.  **Results Packaging**: Returns a comprehensive summary dictionary with all experiment results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "77b6a9a4",
      "metadata": {
        "id": "77b6a9a4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(hidden_size=256, num_layers=1, dropout=0.1, emb_size=128, context_size=5,\n",
        "                   batch_size=128, epochs=10, lr=1e-3, device='cpu', run_name='exp'):\n",
        "\n",
        "    print(f\"Running experiment: {run_name} | hidden={hidden_size} layers={num_layers} dropout={dropout}\")\n",
        "\n",
        "    train_ds, val_ds, token_to_idx, idx_to_token = prepare_dataset(context_size=context_size)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "\n",
        "    model = LSTMNextWordModel(len(idx_to_token), emb_size, hidden_size, num_layers=num_layers, dropout=dropout)\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience, patience_counter = PATIENCE, 0   # stop after 2 epochs without improvement\n",
        "    best_model_state = None\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device) # per epoch loss,acc during training\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device) # per epoch loss,acc during testing\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          best_model_state = model.state_dict()  # save best weights\n",
        "          patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {ep}/{epochs}  train_loss={train_loss:.4f} train_acc={train_acc:.4f}  val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "\n",
        "    if best_model_state:\n",
        "     model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Compute perplexity\n",
        "    val_perplexity = compute_perplexity(val_losses[-1])\n",
        "    print(f\"Validation perplexity: {val_perplexity:.2f}\")\n",
        "\n",
        "    # Save plots\n",
        "    outdir = Path(PATH_RES) / run_name\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Loss plot\n",
        "    plt.figure()\n",
        "    epochs_ran = len(train_losses)\n",
        "    plt.plot(range(1, epochs_ran + 1), train_losses, label='train_loss')\n",
        "    plt.plot(range(1, epochs_ran + 1), val_losses, label='val_loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Cross-Entropy Loss')\n",
        "    plt.legend()\n",
        "    plt.title(f'Loss curves ({run_name})')\n",
        "    plt.savefig(outdir / 'loss.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.figure()\n",
        "    epochs_ran = len(train_accs)\n",
        "    plt.plot(range(1, epochs_ran + 1), train_accs, label='train_acc')\n",
        "    plt.plot(range(1, epochs_ran + 1), val_accs, label='val_acc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title(f'Accuracy curves ({run_name})')\n",
        "    plt.savefig(outdir / 'acc.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Generate text\n",
        "    seed = \"To be or not to\"\n",
        "    generated = generate_text(model, token_to_idx, idx_to_token, seed, gen_len=200, context_size=context_size, device=device, temperature=TEMPERATURE, top_k=TOP_K)\n",
        "    print(f\"Seed: '{seed}'\")\n",
        "    print(\"Generated:\", generated)\n",
        "\n",
        "    # Return summary\n",
        "    return {\n",
        "        'model': model,\n",
        "        'token_to_idx': token_to_idx,\n",
        "        'idx_to_token': idx_to_token,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs,\n",
        "        'val_perplexity': val_perplexity,\n",
        "        'results_dir': str(outdir),\n",
        "        'generated': generated,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x3RV4LFqF4lf",
      "metadata": {
        "id": "x3RV4LFqF4lf"
      },
      "source": [
        "### Ablation Study System\n",
        "Systematic ablation study that runs multiple experiments with different model configurations to analyze the impact of various architectural choices on performance. This function executes a series of controlled experiments where specific model parameters are varied while keeping others constant, allowing for comparative analysis of how different design decisions affect model behavior, training dynamics, and final performance.\n",
        "\n",
        "**Experimental Design**:The function runs multiple experiments with carefully controlled variations:\n",
        "\n",
        "1.  **Hidden Size Comparison**: Tests different model capacities (64 vs 256 units)\n",
        "    \n",
        "2.  **Layer Depth Analysis**: Compares single vs multi-layer architectures\n",
        "    \n",
        "3.  **Dropout Impact**: Evaluates different regularization strengths (0.0, 0.1, 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "82e67bb6",
      "metadata": {
        "id": "82e67bb6"
      },
      "outputs": [],
      "source": [
        "def run_ablation():\n",
        "    experiments = [\n",
        "        # # Hidden size variations\n",
        "        # {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.0, 'run_name': 'hidden_256'},\n",
        "        # {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.0, 'run_name': 'hidden_64'},\n",
        "\n",
        "        # # Number of layers variations\n",
        "        # {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.1, 'run_name': 'layers_1'},\n",
        "        # {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.1, 'run_name': 'layers_2'},\n",
        "\n",
        "        # Dropout variations\n",
        "        # {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.1, 'run_name': 'dropout_0.1'},\n",
        "        # {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.2, 'run_name': 'dropout_0.2'},\n",
        "        # {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.3, 'run_name': 'dropout_0.3'},\n",
        "\n",
        "        {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.1, 'run_name': 'dropout_0.1'},\n",
        "        {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.2, 'run_name': 'dropout_0.2'},\n",
        "        {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.3, 'run_name': 'dropout_0.3'},\n",
        "    ]\n",
        "\n",
        "    summaries = {}\n",
        "    for cfg in experiments:\n",
        "        summary = run_experiment(\n",
        "            hidden_size=cfg['hidden_size'],\n",
        "            num_layers=cfg['num_layers'],\n",
        "            dropout=cfg['dropout'],\n",
        "            emb_size=128,\n",
        "            context_size=5,\n",
        "            batch_size=128,\n",
        "            epochs=20,\n",
        "            lr=1e-3,\n",
        "            device=('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "            run_name=cfg['run_name']\n",
        "        )\n",
        "        summaries[cfg['run_name']] = summary\n",
        "\n",
        "    # Plot comparison of validation loss curves\n",
        "    plt.figure()\n",
        "    for name, s in summaries.items():\n",
        "        plt.plot(range(1, len(s['val_losses']) + 1), s['val_losses'], label=f'{name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Val Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Ablation: val loss by config')\n",
        "    comp_dir = Path(PATH_RES) / 'ablation'\n",
        "    comp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(comp_dir / 'ablation_val_loss.png')\n",
        "    plt.close()\n",
        "\n",
        "    print('Ablation results saved to', comp_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cabe56f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cabe56f",
        "outputId": "df284a94-8f40-47d4-a160-cccf5b434728"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 128\n",
        "ablation = True   # set to True if you want to run the ablation study\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if ablation:\n",
        "    run_ablation()\n",
        "else:\n",
        "    run_experiment(\n",
        "        hidden_size=256,\n",
        "        num_layers=1,\n",
        "        dropout=0.1,\n",
        "        emb_size=128,\n",
        "        context_size=5,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        lr=1e-3,\n",
        "        device=device,\n",
        "        run_name=PATH_RES\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
