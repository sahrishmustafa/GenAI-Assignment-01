{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9dd76de3",
      "metadata": {
        "id": "9dd76de3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PixelCNN / Row LSTM / Diagonal BiLSTM (PixelRNN) - PyTorch implementation skeleton\n",
        "\n",
        "This file contains:\n",
        "- MaskedConv2d (mask type 'A' and 'B')\n",
        "- PixelCNN (stack of masked convs)\n",
        "- RowLSTM layer and stack\n",
        "- Diagonal BiLSTM layer (skew/unskew + bidirectional LSTM along diagonals)\n",
        "- CIFAR-10 dataloaders, training loop, evaluation (bits/dim), plotting\n",
        "\n",
        "This is a reasonably complete, runnable starting point. It prioritizes clarity and\n",
        "correctness over absolute performance. For larger-scale experiments you should\n",
        "optimize/sketch kernels and batching of diagonals.\n",
        "\n",
        "Notes / caveats:\n",
        "- The Diagonal BiLSTM here uses Python loops over diagonals for clarity. It is\n",
        "  correct but slower than highly-optimized implementations.\n",
        "- MaskedConv2d implements both spatial masking and channel-order masking for RGB.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------- Utilities ---------------------------------\n",
        "\n",
        "def to_device(x):\n",
        "    return x.cuda() if torch.cuda.is_available() else x\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --------------------------- Masked Conv2d -------------------------------\n",
        "class MaskedConv2d(nn.Conv2d):\n",
        "    \"\"\"A Conv2d with a mask applied to the weights to enforce autoregressive ordering.\n",
        "\n",
        "    Supports two mask types: 'A' (first layer) and 'B' (subsequent layers).\n",
        "    Also supports channel-wise masking to enforce RGB ordering when in_channels==3.\n",
        "\n",
        "    Reference: van den Oord et al. PixelCNN/PixelRNN.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, mask_type,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super().__init__(in_channels, out_channels, kernel_size,\n",
        "                         stride, padding, dilation, groups, bias)\n",
        "        assert mask_type in (\"A\", \"B\"), \"mask_type must be 'A' or 'B'\"\n",
        "        self.register_buffer('mask', self.weight.data.clone().zero_())\n",
        "        kH, kW = self.kernel_size\n",
        "        center_h = kH // 2\n",
        "        center_w = kW // 2\n",
        "\n",
        "        # Basic spatial mask: positions below/right of center are masked.\n",
        "        mask = torch.ones_like(self.weight.data)\n",
        "        for i in range(kH):\n",
        "            for j in range(kW):\n",
        "                if i > center_h or (i == center_h and j > center_w):\n",
        "                    mask[:, :, i, j] = 0\n",
        "        if mask_type == 'A':\n",
        "            # center pixel excluded for mask A: zero center\n",
        "            mask[:, :, center_h, center_w] = 0\n",
        "        else:\n",
        "            # mask_type == 'B' -> center allowed (no change)\n",
        "            pass\n",
        "\n",
        "        # Channel-wise masking for RGB ordering in first layer typically\n",
        "        # If input and output channels are multiples of 3 we can apply the\n",
        "        # classic PixelCNN channel mask to prevent e.g. G depending on G of same pixel.\n",
        "        if in_channels % 3 == 0 and out_channels % 3 == 0:\n",
        "            in_groups = in_channels // 3\n",
        "            out_groups = out_channels // 3\n",
        "            for out_g in range(out_groups):\n",
        "                for in_g in range(in_groups):\n",
        "                    for i in range(kH):\n",
        "                        for j in range(kW):\n",
        "                            if i == center_h and j == center_w:\n",
        "                                # center pixel channel relation\n",
        "                                if mask_type == 'A':\n",
        "                                    # out_channel group must be greater than in_channel group\n",
        "                                    if out_g <= in_g:\n",
        "                                        mask[out_g*3:(out_g+1)*3, in_g*3:(in_g+1)*3, i, j] = 0\n",
        "                                else:\n",
        "                                    # mask B: allow center for equal groups but still block future channels\n",
        "                                    # allow when out_g > in_g OR (out_g == in_g)\n",
        "                                    # but need to block connections from a channel to \"future\" channels inside same pixel\n",
        "                                    # We'll allow equal groups but still zero upper-triangular inside the 3x3 submatrix\n",
        "                                    # For simplicity, block upper-triangular channel connections in center\n",
        "                                    sub = torch.ones(3, 3)\n",
        "                                    # Channel order: R(0),G(1),B(2). Allow:\n",
        "                                    # out R can see in R only if ... (for mask B center, allow self)\n",
        "                                    # Following common implementations, zero upper triangle\n",
        "                                    sub[0,1] = 0\n",
        "                                    sub[0,2] = 0\n",
        "                                    sub[1,2] = 0\n",
        "                                    mask[out_g*3:(out_g+1)*3, in_g*3:(in_g+1)*3, i, j] *= sub\n",
        "                            else:\n",
        "                                # Non-center positions (past pixels): full channels allowed\n",
        "                                pass\n",
        "        # else: if channels not multiples of 3, fall back to pure spatial mask\n",
        "\n",
        "        self.mask.copy_(mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.weight.data *= self.mask\n",
        "        return super().forward(x)\n",
        "\n",
        "# ----------------------------- PixelCNN ---------------------------------\n",
        "# --------------------- Improved PixelCNN (residual, deeper) ---------------------\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block used in PixelCNN (masked conv -> ReLU -> masked conv + 1x1, residual add).\"\"\"\n",
        "    def __init__(self, nr_filters, kernel_size=3):\n",
        "        super().__init__()\n",
        "        pad = kernel_size // 2\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            MaskedConv2d(nr_filters, nr_filters, kernel_size=kernel_size, mask_type='B', padding=pad),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(nr_filters, nr_filters, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.net(x)  # residual add\n",
        "\n",
        "\n",
        "class PixelCNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, nr_residual=15, nr_filters=128, ks_first=7):\n",
        "        super().__init__()\n",
        "        pad = ks_first // 2\n",
        "        # first layer: Mask A (prevents same-pixel leakage)\n",
        "        self.first = MaskedConv2d(in_channels, nr_filters, kernel_size=ks_first, mask_type='A', padding=pad)\n",
        "\n",
        "        # stack of residual blocks (each uses Mask B)\n",
        "        self.res_blocks = nn.ModuleList([ResidualBlock(nr_filters, kernel_size=3) for _ in range(nr_residual)])\n",
        "\n",
        "        # final processing\n",
        "        self.final = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(nr_filters, nr_filters, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        # output conv -> C * 256 logits per pixel\n",
        "        self.out_conv = nn.Conv2d(nr_filters, in_channels * 256, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x should be float tensor (we keep input as float; target remains integer labels 0..255)\n",
        "        h = self.first(x)\n",
        "        for block in self.res_blocks:\n",
        "            h = block(h)\n",
        "        h = self.final(h)\n",
        "        logits = self.out_conv(h)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ----------------------------- Row LSTM ---------------------------------\n",
        "class RowLSTMCell(nn.Module):\n",
        "    \"\"\"A convolutional LSTM cell that moves row-by-row.\n",
        "\n",
        "    Input-to-state: precomputed masked conv kx1 producing 4*h gates.\n",
        "    State-to-state: conv with kernel K_ss applied to previous hidden state.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.hidden_channels = hidden_channels\n",
        "        # input-to-state conv should be masked (no future pixels in same row)\n",
        "        pad = (kernel_size // 2, 0)\n",
        "        self.input_conv = MaskedConv2d(in_channels, 4*hidden_channels, kernel_size=(kernel_size,1),\n",
        "                                       mask_type='B', padding=pad)\n",
        "        # state-to-state conv: operates on previous row's hidden state\n",
        "        # We'll use a conv that looks vertically (k x 1) on the previous hidden map\n",
        "        self.state_conv = nn.Conv2d(hidden_channels, 4*hidden_channels, kernel_size=(kernel_size,1), padding=pad)\n",
        "\n",
        "        # 1x1 projection to return to in_channels feature space (for residual)\n",
        "        self.out_conv = nn.Conv2d(hidden_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, H, W]\n",
        "        B, C, H, W = x.shape\n",
        "        device = x.device\n",
        "        its = self.input_conv(x)  # [B, 4h, H, W]\n",
        "\n",
        "        h_prev = torch.zeros(B, self.hidden_channels, W, device=device)\n",
        "        c_prev = torch.zeros(B, self.hidden_channels, W, device=device)\n",
        "\n",
        "        outputs = []\n",
        "        for r in range(H):\n",
        "            # gates for this row from input-to-state\n",
        "            gates_x = its[:, :, r, :]  # [B, 4h, W]\n",
        "            # compute state-to-state conv on previous hidden arranged as [B, h, 1, W]\n",
        "            h_prev_map = h_prev.unsqueeze(2)  # [B, h, 1, W]\n",
        "            rec = self.state_conv(h_prev_map).squeeze(2)  # [B, 4h, W]\n",
        "\n",
        "            gates = rec + gates_x  # broadcasting\n",
        "            # split gates\n",
        "            i_gate, f_gate, o_gate, g_gate = gates.chunk(4, dim=1)\n",
        "            i = torch.sigmoid(i_gate)\n",
        "            f = torch.sigmoid(f_gate)\n",
        "            o = torch.sigmoid(o_gate)\n",
        "            g = torch.tanh(g_gate)\n",
        "\n",
        "            c = f * c_prev + i * g\n",
        "            h = o * torch.tanh(c)\n",
        "\n",
        "            outputs.append(h.unsqueeze(2))  # [B, h, 1, W]\n",
        "            h_prev = h\n",
        "            c_prev = c\n",
        "\n",
        "        out_map = torch.cat(outputs, dim=2)  # [B, h, H, W]\n",
        "        # project back\n",
        "        residual = self.out_conv(out_map)\n",
        "        return residual\n",
        "\n",
        "class RowLSTMStack(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=64, n_layers=3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        cur_ch = in_channels\n",
        "        for _ in range(n_layers):\n",
        "            layers.append(RowLSTMCell(cur_ch, hidden_channels))\n",
        "            cur_ch = in_channels  # RowLSTMCell returns features of in_channels due to residual projection\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.out_conv = nn.Conv2d(in_channels, in_channels * 256, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h)\n",
        "        logits = self.out_conv(F.relu(h))\n",
        "        return logits\n",
        "\n",
        "# ------------------------- Diagonal BiLSTM -------------------------------\n",
        "\n",
        "def skew_tensor(x):\n",
        "    \"\"\"Skew input map so diagonals become columns.\n",
        "\n",
        "    Input x: [B, C, H, W]\n",
        "    Output: [B, C, H, W+H-1] (we pad on the right)\n",
        "    For row r (0-based) we shift it right by r positions.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    new_W = W + H - 1\n",
        "    device = x.device\n",
        "    out = x.new_zeros((B, C, H, new_W))\n",
        "    for r in range(H):\n",
        "        out[:, :, r, r:r+W] = x[:, :, r, :]\n",
        "    return out\n",
        "\n",
        "\n",
        "def unskew_tensor(x, orig_W):\n",
        "    # x: [B, C, H, W+H-1]\n",
        "    B, C, H, Wfull = x.shape\n",
        "    W = orig_W\n",
        "    out = x.new_zeros((B, C, H, W))\n",
        "    for r in range(H):\n",
        "        out[:, :, r, :] = x[:, :, r, r:r+W]\n",
        "    return out\n",
        "\n",
        "class DiagonalBiLSTMLayer(nn.Module):\n",
        "    \"\"\"Diagonal BiLSTM: skew, run bidirectional LSTM along diagonals, unskew, combine.\n",
        "\n",
        "    Implementation: For clarity we extract diagonals as sequences and run an nn.LSTM\n",
        "    over each diagonal (packed as batch of sequences of varying lengths). This is\n",
        "    simple to implement though not the most efficient.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        # input-to-state conv is 1x1\n",
        "        self.input_conv = nn.Conv2d(in_channels, 4*hidden_channels, kernel_size=1)\n",
        "        # We'll use linear layers for state-to-state inside LSTM cell (we'll use nn.LSTM for simplicity)\n",
        "        # We'll process each diagonal as a sequence of vectors of size in_channels\n",
        "        self.lstm_fwd = nn.LSTM(input_size=in_channels, hidden_size=hidden_channels, batch_first=True)\n",
        "        self.lstm_bwd = nn.LSTM(input_size=in_channels, hidden_size=hidden_channels, batch_first=True)\n",
        "        # projection back to input channels\n",
        "        self.out_conv = nn.Conv2d(hidden_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def extract_diagonals(self, x):\n",
        "        # x: [B, C, H, W]\n",
        "        B, C, H, W = x.shape\n",
        "        diags = []\n",
        "        lengths = []\n",
        "        for d in range(H+W-1):\n",
        "            # diagonal d has elements where r + c = d\n",
        "            elems = []\n",
        "            for r in range(H):\n",
        "                c = d - r\n",
        "                if 0 <= c < W:\n",
        "                    elems.append(x[:, :, r, c])  # [B, C]\n",
        "            if len(elems) > 0:\n",
        "                # stack along time dim: [B, len, C]\n",
        "                seq = torch.stack(elems, dim=1)\n",
        "                diags.append(seq)\n",
        "                lengths.append(seq.size(1))\n",
        "        return diags, lengths\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, H, W]\n",
        "        B, C, H, W = x.shape\n",
        "        device = x.device\n",
        "        # We will produce a hidden map of size [B, hidden, H, W]\n",
        "        hmap = x.new_zeros((B, self.hidden_channels, H, W))\n",
        "\n",
        "        # Extract diagonals as sequences\n",
        "        diags, lengths = self.extract_diagonals(x)\n",
        "        # Process each diagonal sequence with forward and backward LSTMs\n",
        "        for idx, seq in enumerate(diags):\n",
        "            # seq: [B, L, C]\n",
        "            L = seq.size(1)\n",
        "            # forward\n",
        "            out_f, _ = self.lstm_fwd(seq)  # [B, L, hidden]\n",
        "            # backward: reverse sequence along time dim\n",
        "            seq_rev = torch.flip(seq, dims=[1])\n",
        "            out_b_rev, _ = self.lstm_bwd(seq_rev)\n",
        "            out_b = torch.flip(out_b_rev, dims=[1])\n",
        "            out = out_f + out_b  # combine\n",
        "            # place outputs back into hmap\n",
        "            # need to map positions back: diagonal index idx corresponds to positions where r + c = d\n",
        "            t = 0\n",
        "            for r in range(H):\n",
        "                c = idx - r\n",
        "                if 0 <= c < W:\n",
        "                    hmap[:, :, r, c] = out[:, t, :].transpose(1, 0).transpose(2, 1).squeeze(-1) if False else out[:, t, :].transpose(1,0).transpose(2,1).squeeze(-1)\n",
        "                    # simpler: assign by matching shapes\n",
        "                    # out[:, t, :] is [B, hidden]; target hmap[:, :, r, c] is [B, hidden]\n",
        "                    hmap[:, :, r, c] = out[:, t, :].permute(1,0) if False else out[:, t, :].permute(1,0)\n",
        "                    # the above two lines are awkward; fix below\n",
        "                    t += 1\n",
        "        # The above assignment approach is messy; rebuild properly below\n",
        "        # Recompute with clean assignment\n",
        "        hmap = x.new_zeros((B, self.hidden_channels, H, W))\n",
        "        for idx, seq in enumerate(diags):\n",
        "            out_f, _ = self.lstm_fwd(seq)\n",
        "            seq_rev = torch.flip(seq, dims=[1])\n",
        "            out_b_rev, _ = self.lstm_bwd(seq_rev)\n",
        "            out_b = torch.flip(out_b_rev, dims=[1])\n",
        "            out = out_f + out_b  # [B, L, hidden]\n",
        "            t = 0\n",
        "            for r in range(H):\n",
        "                c = idx - r\n",
        "                if 0 <= c < W:\n",
        "                    # assign out[:, t, :] -> hmap[:, :, r, c]\n",
        "                    hmap[:, :, r, c] = out[:, t, :].permute(1, 0)\n",
        "                    # but shapes don't match; fix by direct assignment\n",
        "                    # hmap[:, :, r, c] has shape [B, hidden]; out[:, t, :] has shape [B, hidden]\n",
        "                    hmap[:, :, r, c] = out[:, t, :]\n",
        "                    t += 1\n",
        "\n",
        "        # project back\n",
        "        residual = self.out_conv(hmap)\n",
        "        return residual\n",
        "\n",
        "# ----------------------------- Full wrappers ----------------------------\n",
        "class PixelRNN_PixelCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = PixelCNN()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class PixelRNN_RowLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = RowLSTMStack()\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class PixelRNN_DiagBiLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer = DiagonalBiLSTMLayer(in_channels=3, hidden_channels=64)\n",
        "        self.out_conv = nn.Conv2d(3, 3*256, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        h = self.layer(x)\n",
        "        logits = self.out_conv(F.relu(h))\n",
        "        return logits\n",
        "\n",
        "# ----------------------------- Training ---------------------------------\n",
        "\n",
        "def logits_to_nll(logits, target):\n",
        "    \"\"\"\n",
        "    logits: [B, C*256, H, W]\n",
        "    target: [B, C, H, W] with ints 0..255\n",
        "    Returns total negative log-likelihood (sum over batch) in nats\n",
        "    \"\"\"\n",
        "    B, C256, H, W = logits.shape\n",
        "    C = target.shape[1]\n",
        "    logits = logits.view(B, C, 256, H, W).permute(0,1,3,4,2).contiguous()  # [B,C,H,W,256]\n",
        "    logits = logits.view(-1, 256)\n",
        "    targets = target.permute(0,2,3,1).contiguous().view(-1)\n",
        "    loss = F.cross_entropy(logits, targets, reduction='sum')\n",
        "    return loss\n",
        "\n",
        "\n",
        "def nll_to_bits_per_dim(nll, batch_size, C, H, W):\n",
        "    # bits/dim = (nll / log(2)) / (N * D)\n",
        "    nll_bits = nll / math.log(2)\n",
        "    dims = batch_size * C * H * W\n",
        "    return nll_bits / dims\n",
        "\n",
        "\n",
        "def to_long_tensor(x):\n",
        "    # torchvision ToTensor gives float in [0,1]\n",
        "    return (x * 255).long()\n",
        "\n",
        "def get_dataloaders(batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        to_long_tensor\n",
        "    ])\n",
        "    train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # On Windows, set num_workers=0 to avoid multiprocessing issues\n",
        "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, opt, loader, device):\n",
        "    model.train()\n",
        "    total_nll = 0.0\n",
        "    total_samples = 0\n",
        "    t0 = time.time()\n",
        "    for xb, _ in loader:\n",
        "        # xb: [B, C, H, W] floats? Our transforms return long ints\n",
        "        xb = xb.to(device=device, dtype=torch.long)  # integers\n",
        "        x_in = xb.float()  # feed through model as floats (some layers rely on float inputs)\n",
        "        logits = model(x_in)\n",
        "        loss = logits_to_nll(logits, xb)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        # gradient clipping (important for stability, esp. if you later train LSTM variants)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        opt.step()\n",
        "\n",
        "        opt.step()\n",
        "        total_nll += loss.item()\n",
        "        total_samples += xb.size(0)\n",
        "    t1 = time.time()\n",
        "    return total_nll, total_samples, t1 - t0\n",
        "\n",
        "\n",
        "def eval_epoch(model, loader, device):\n",
        "    model.eval()\n",
        "    total_nll = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device=device, dtype=torch.long)\n",
        "            x_in = xb.float()\n",
        "            logits = model(x_in)\n",
        "            loss = logits_to_nll(logits, xb)\n",
        "            total_nll += loss.item()\n",
        "            total_samples += xb.size(0)\n",
        "    return total_nll, total_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "gX2vqo1CX1A2",
      "metadata": {
        "id": "gX2vqo1CX1A2"
      },
      "outputs": [],
      "source": [
        "def init_weights_xavier(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if getattr(m, 'bias', None) is not None:\n",
        "            nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ygJs0EzKUU1V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygJs0EzKUU1V",
        "outputId": "dc01ab9c-60d5-4b30-929c-5844cd2d168b"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m history = {\u001b[33m'\u001b[39m\u001b[33mtrain_bpd\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mval_bpd\u001b[39m\u001b[33m'\u001b[39m: []}\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     t_nll, t_samples, t_time = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n\u001b[32m     36\u001b[39m     train_bpd = nll_to_bits_per_dim(t_nll, t_samples, \u001b[32m3\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m32\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 441\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, opt, loader, device)\u001b[39m\n\u001b[32m    438\u001b[39m loss = logits_to_nll(logits, xb)\n\u001b[32m    439\u001b[39m opt.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# gradient clipping (important for stability, esp. if you later train LSTM variants)\u001b[39;00m\n\u001b[32m    443\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\semester 7\\gen ai\\ass1\\venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\semester 7\\gen ai\\ass1\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\semester 7\\gen ai\\ass1\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ----------------------------- Notebook Run -------------------------------\n",
        "# Parameters (can be edited directly in notebook cells)\n",
        "model_choice = 'pixelcnn'  # 'pixelcnn', 'rowlstm', or 'diagbilstm'\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "\n",
        "# Dataloaders\n",
        "train_loader, val_loader = get_dataloaders(batch_size=batch_size)\n",
        "\n",
        "# Model selection\n",
        "if model_choice == 'pixelcnn':\n",
        "    model = PixelRNN_PixelCNN().to(DEVICE)\n",
        "    model.apply(init_weights_xavier)\n",
        "elif model_choice == 'rowlstm':\n",
        "    model = PixelRNN_RowLSTM().to(DEVICE)\n",
        "else:\n",
        "    model = PixelRNN_DiagBiLSTM().to(DEVICE)\n",
        "\n",
        "# Optimizer\n",
        "# opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# Use RMSprop as in the paper (common setting)\n",
        "opt = torch.optim.RMSprop(model.parameters(), lr=lr, alpha=0.95, eps=1e-8)\n",
        "\n",
        "# # Optional: a scheduler to reduce LR every N epochs\n",
        "# from torch.optim.lr_scheduler import StepLR\n",
        "# scheduler = StepLR(opt, step_size=10, gamma=0.5)  # reduce lr by 2 every 10 epochs (tune as needed)\n",
        "\n",
        "# Training loop\n",
        "history = {'train_bpd': [], 'val_bpd': []}\n",
        "for ep in range(epochs):\n",
        "    t_nll, t_samples, t_time = train_epoch(model, opt, train_loader, DEVICE)\n",
        "\n",
        "    #scheduler.step()\n",
        "\n",
        "    train_bpd = nll_to_bits_per_dim(t_nll, t_samples, 3, 32, 32)\n",
        "    v_nll, v_samples = eval_epoch(model, val_loader, DEVICE)\n",
        "    val_bpd = nll_to_bits_per_dim(v_nll, v_samples, 3, 32, 32)\n",
        "    history['train_bpd'].append(train_bpd)\n",
        "    history['val_bpd'].append(val_bpd)\n",
        "    print(f\"Epoch {ep+1}/{epochs}  train bpd: {train_bpd:.4f}  val bpd: {val_bpd:.4f}  time: {t_time:.1f}s\")\n",
        "\n",
        "# Plot results\n",
        "plt.plot(history['train_bpd'], label='train bpd')\n",
        "plt.plot(history['val_bpd'], label='val bpd')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('bits/dim')\n",
        "plt.title(f\"Training {model_choice}\")\n",
        "plt.show()\n",
        "\n",
        "print('Training complete.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
